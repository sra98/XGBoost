# XGBoost 
XGBoost is a decision-tree-based ensemble Machine Learning algorithm. In prediction problems involving unstructured data like images, text, etc., artificial neural networks tend to outperform all other frameworks. Here, model is created through multiple iterations.
Usually in boosting, we build models by minimizing the error from previous models with increased influence of high performing models. Thus errors are errors are observed first and made input to succeeding model to predict new value. This cycle continues till the ensemble model is obtained. This is how by increasing influence of high performing models and minimizing errors, ensemble is obtained through boosting.
Here, implementation of model supports the features of scikit learn and R implementations. The main three forms of gradient boosting â€“ gradient boosting, stochastic gradient boosting and regularized gradient boosting are supported. XGBoost is widely used for classification problems and can handle missing values without an imputation preprocessing.
The implementation of the algorithm was engineered for efficiency of compute time and memory resources. Thus main two reasons to choose XGBoost are execution speed and model performance. XGBoost is really fast as compared to other implementations of gradient boosting. XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems. 
